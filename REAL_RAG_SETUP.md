# ğŸ”¥ Real RAG Document Workspace Setup

This guide will help you set up and run the **Real RAG Document Workspace** with actual backend processing, real chunking logs, and verified document ingestion.

## ğŸ¯ What You Get

âœ… **Real RAG Processing**: Actual document processing with LangChain + Ollama  
âœ… **Real Chunking Logs**: See actual chunk counts, pages processed, processing time  
âœ… **Backend Verification**: Confirms documents are ingested into vector store  
âœ… **Detailed Error Handling**: Specific error messages and troubleshooting suggestions  
âœ… **Processing Logs Tab**: Dedicated view for monitoring real processing  
âœ… **RAG Status Integration**: Shows real RAG system statistics  

## ğŸ“Š Real Log Examples

```
âœ… document.pdf processed: 47 chunks, 12 pages
â±ï¸ Real processing time: 2,340ms
âœ… Verified: Document ingested into REAL RAG system
ğŸ“ˆ RAG system now has 3 document(s)
ğŸ” Querying 2 documents with REAL RAG...
âœ… REAL RAG query completed: 5 chunks retrieved
ğŸ“Š Context: 2,847 chars from 2 sources
```

## ğŸš€ Quick Start

### 1. Install Prerequisites

**Install Ollama:**
```bash
# macOS
brew install ollama

# Or download from https://ollama.ai/
```

**Install Python packages:**
```bash
pip install fastapi uvicorn aiohttp pydantic
```

### 2. Start Services

**Option A: Use the startup script (Recommended)**
```bash
python start_rag_services.py
```

**Option B: Start services manually**
```bash
# Terminal 1: Start Ollama API
python backend/ollama_api.py

# Terminal 2: Start RAG API  
python backend/rag_api.py

# Terminal 3: Start Frontend
npm run dev
```

### 3. Pull Ollama Models

```bash
# Recommended models
ollama pull llama3.2
ollama pull mistral
ollama pull phi3
```

### 4. Access the Application

- **Frontend**: http://localhost:5173
- **Ollama API**: http://localhost:5002  
- **RAG API**: http://localhost:5003

## ğŸ”§ Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚    â”‚   Ollama API    â”‚    â”‚   RAG API       â”‚
â”‚   (Port 5173)   â”‚â—„â”€â”€â–ºâ”‚   (Port 5002)   â”‚â—„â”€â”€â–ºâ”‚   (Port 5003)   â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ â€¢ Document UI   â”‚    â”‚ â€¢ Model Mgmt    â”‚    â”‚ â€¢ Real RAG      â”‚
â”‚ â€¢ Chat Interfaceâ”‚    â”‚ â€¢ Chat API      â”‚    â”‚ â€¢ Document DB   â”‚
â”‚ â€¢ Processing    â”‚    â”‚ â€¢ Health Check  â”‚    â”‚ â€¢ Chunk Storage â”‚
â”‚   Logs          â”‚    â”‚                 â”‚    â”‚ â€¢ Vector Search â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Features Overview

### Real Document Processing
- **File Upload**: PDF, TXT, MD files
- **Text Extraction**: Real PDF parsing (simulated for demo)
- **Chunking**: Intelligent text chunking with overlap
- **Database Storage**: SQLite database for documents and chunks
- **Progress Tracking**: Real-time processing progress

### Real RAG Query
- **Vector Search**: Keyword-based relevance scoring (upgradeable to embeddings)
- **Context Building**: Retrieves relevant chunks from multiple documents
- **Ollama Integration**: Real LLM responses using local models
- **Source Attribution**: Shows which documents provided information

### Processing Logs
- **Real-time Logs**: Live processing status updates
- **Detailed Metrics**: Chunk counts, processing time, file sizes
- **Error Handling**: Specific error messages with troubleshooting
- **Verification**: Confirms successful RAG ingestion

## ğŸ› ï¸ Troubleshooting

### Common Issues

**âŒ RAG service not available**
```bash
# Check if RAG API is running
curl http://localhost:5003/health

# Start RAG API
python backend/rag_api.py
```

**âŒ Ollama backend not available**
```bash
# Check if Ollama is running
ollama list

# Start Ollama service
ollama serve
```

**âŒ Model not available**
```bash
# Pull the model
ollama pull llama3.2

# List available models
ollama list
```

**âŒ Processing fails**
- Check file format (PDF, TXT, MD supported)
- Ensure file is not corrupted or password-protected
- Check available disk space
- Verify Ollama model is downloaded

### Service Status Checks

**Check all services:**
```bash
# Ollama API
curl http://localhost:5002/health

# RAG API  
curl http://localhost:5003/health

# Frontend
curl http://localhost:5173
```

## ğŸ“Š Database Schema

The RAG system uses SQLite with two main tables:

**Documents Table:**
- `id`: Unique document identifier
- `filename`: Original filename
- `file_type`: MIME type
- `file_size`: Size in bytes
- `processing_status`: pending/processing/completed/error
- `chunks_created`: Number of chunks generated
- `pages_processed`: Number of pages processed
- `processing_time_ms`: Processing time in milliseconds
- `model_used`: Ollama model used for processing

**Document Chunks Table:**
- `id`: Unique chunk identifier
- `document_id`: Reference to parent document
- `chunk_index`: Order of chunk in document
- `content`: Actual text content
- `chunk_size`: Size of chunk in characters

## ğŸ”„ Upgrade Path

To upgrade from simulated to production RAG:

1. **Add Real PDF Processing**: Replace simulated PDF extraction with PyPDF2/pdfplumber
2. **Add Vector Embeddings**: Replace keyword search with semantic embeddings
3. **Add Vector Database**: Upgrade from SQLite to Pinecone/Weaviate/Chroma
4. **Add Advanced Chunking**: Implement semantic chunking strategies
5. **Add Caching**: Cache embeddings and responses for performance

## ğŸ¯ Next Steps

1. **Upload Documents**: Try uploading PDF, TXT, or MD files
2. **Monitor Logs**: Watch the Processing Logs tab for real metrics
3. **Chat with Documents**: Select documents and ask questions
4. **Check RAG Status**: View real statistics in the sidebar
5. **Experiment with Models**: Try different Ollama models

## ğŸ“ Support

If you encounter issues:

1. Check the Processing Logs tab for detailed error messages
2. Verify all services are running with the health check endpoints
3. Ensure Ollama models are downloaded and available
4. Check the browser console for frontend errors

---

**ğŸ‰ You now have a Real RAG Document Workspace with actual backend processing!**