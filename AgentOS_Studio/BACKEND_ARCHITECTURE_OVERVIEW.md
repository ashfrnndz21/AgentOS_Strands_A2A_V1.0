# ğŸ—ï¸ Backend Architecture & Port Configuration

## Current Backend Services

Our system now runs **multiple independent backend services** on different ports, each serving specific purposes:

### ğŸ“Š Service Port Map

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend Services                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Port 5002 â”‚ ollama_api.py          â”‚ Legacy Ollama Agents   â”‚
â”‚ Port 5003 â”‚ strands_api.py         â”‚ Custom Strands System  â”‚
â”‚ Port 5004 â”‚ chat_orchestrator_api.pyâ”‚ Chat Orchestration    â”‚
â”‚ Port 5005 â”‚ rag_api.py             â”‚ RAG Document System    â”‚
â”‚ Port 5006 â”‚ strands_sdk_api.py     â”‚ Official Strands SDK   â”‚ â† NEW!
â”‚ Port 11434â”‚ Ollama Server          â”‚ LLM Model Server       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ†• New Strands SDK Service (Port 5006)

### Purpose
- **Official Strands SDK Integration**: Uses the real `strands-agents` Python package
- **Parallel System**: Runs alongside existing services without disruption
- **SDK Compliance**: Follows official Strands patterns exactly

### Key Features
```python
# Real Strands SDK Usage
from strands import Agent
from strands.model_providers import OllamaModel

# Create agent following official patterns
ollama_model = OllamaModel(
    host="http://localhost:11434",
    model_id="llama3"
)

agent = Agent(
    model=ollama_model,
    system_prompt="You are a helpful assistant."
)

# Execute agent
response = agent("Your input here")
```

### Database
- **Separate Database**: `strands_sdk_agents.db` (independent from existing systems)
- **Tables**: 
  - `strands_sdk_agents` - Agent configurations
  - `strands_sdk_executions` - Execution logs with SDK metadata

### API Endpoints
```
GET    /api/strands-sdk/health           # Health check
POST   /api/strands-sdk/agents          # Create agent
GET    /api/strands-sdk/agents          # List agents
GET    /api/strands-sdk/agents/{id}     # Get specific agent
POST   /api/strands-sdk/agents/{id}/execute  # Execute agent
DELETE /api/strands-sdk/agents/{id}     # Delete agent
```

## ğŸ¨ Frontend Integration

### Dashboard Integration
The `OllamaAgentDashboard.tsx` now has **two creation buttons**:

```tsx
// Existing button (unchanged)
<Button onClick={() => setShowCreateDialog(true)}>
  <Plus size={16} className="mr-2" />
  Create Agent
</Button>

// NEW Strands SDK button
<Button onClick={() => setShowStrandsSdkDialog(true)}>
  <Sparkles size={16} className="mr-2" />
  Create Strands Agent
</Button>
```

### Service Layer
```typescript
// src/lib/services/StrandsSdkService.ts
class StrandsSdkService {
  private baseUrl = 'http://localhost:5006/api/strands-sdk';
  
  async createAgent(agentData: StrandsSdkCreateRequest) {
    // Communicates with port 5006
  }
}
```

### UI Components
```
src/components/MultiAgentWorkspace/
â”œâ”€â”€ StrandsSdkAgentDialog.tsx     # Creation dialog with SDK preview
â””â”€â”€ (existing components unchanged)

src/lib/services/
â”œâ”€â”€ StrandsSdkService.ts          # New service for SDK API
â”œâ”€â”€ OllamaAgentService.ts         # Existing service (unchanged)
â””â”€â”€ (other services unchanged)
```

## ğŸ”„ How It All Works Together

### 1. User Workflow
```
User clicks "Create Strands Agent" 
    â†“
StrandsSdkAgentDialog opens
    â†“
Shows form + real SDK code preview
    â†“
User fills form and clicks create
    â†“
StrandsSdkService.createAgent() called
    â†“
HTTP POST to localhost:5006/api/strands-sdk/agents
    â†“
Backend creates agent using real Strands SDK
    â†“
Agent stored in strands_sdk_agents.db
    â†“
Success response returned to UI
    â†“
Dialog closes, dashboard refreshes
```

### 2. Agent Execution Flow
```
User executes Strands SDK agent
    â†“
StrandsSdkService.executeAgent() called
    â†“
HTTP POST to localhost:5006/api/strands-sdk/agents/{id}/execute
    â†“
Backend loads agent config from database
    â†“
Creates Strands Agent using official SDK:
  - OllamaModel(host, model_id)
  - Agent(model, system_prompt)
    â†“
Executes: agent(input_text)
    â†“
Logs execution in strands_sdk_executions table
    â†“
Returns response with SDK metadata
```

### 3. Mixed Agent Display
The dashboard shows **both types** of agents with clear badges:

```tsx
// Legacy agents show:
<Badge variant="secondary">Legacy</Badge>

// Strands SDK agents show:
<Badge variant="secondary" className="bg-purple-100 text-purple-700">
  <Sparkles className="h-3 w-3 mr-1" />
  Strands SDK
</Badge>
```

## ğŸš€ Benefits of This Architecture

### âœ… Zero Risk
- Existing system completely untouched
- Independent databases and services
- Easy to remove if needed

### âœ… True SDK Compliance
- Uses official `strands-agents` package
- Follows documented patterns exactly
- Gets official SDK updates automatically

### âœ… User Choice
- Side-by-side comparison of approaches
- Users can choose preferred method
- Gradual migration path available

### âœ… Future-Proof
- Foundation for full SDK migration
- Can add advanced SDK features incrementally
- Maintains backward compatibility

## ğŸ”§ Service Management

### Start All Services
```bash
# Start individual services
./start-strands-sdk-service.sh     # Port 5006
./start-all-services.sh            # All other services

# Or start specific service
cd backend && python3 strands_sdk_api.py
```

### Health Checks
```bash
# Check Strands SDK service
curl http://localhost:5006/api/strands-sdk/health

# Check all services
./check-services-status.sh
```

### Service Dependencies
```
Strands SDK Service (5006)
    â†“ depends on
Ollama Server (11434)
    â†“ provides
LLM Models (llama3, mistral, etc.)
```

## ğŸ“Š Data Flow Summary

```
Frontend (React/TypeScript)
    â†• HTTP Requests
StrandsSdkService.ts
    â†• Port 5006
strands_sdk_api.py (Flask)
    â†• Official SDK
Strands Agent Library
    â†• Port 11434
Ollama Server
    â†• Model Loading
Local LLM Models
```

This architecture gives us the best of both worlds: **stability** (existing system unchanged) and **innovation** (official Strands SDK integration) running in parallel!