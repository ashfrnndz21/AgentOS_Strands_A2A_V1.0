# ğŸ¯ Chat Orchestrator Architecture

## ğŸš€ **New Intelligent Chat Backend**

The Chat Orchestrator is a dedicated backend service that provides **intelligent chat routing and agent orchestration** for the Strands platform. It replaces the simple proxy approach with a sophisticated system that understands context and routes conversations intelligently.

---

## ğŸ—ï¸ **Architecture Overview**

```
Frontend (React)
    â†“
ChatOrchestratorService.ts
    â†“
Chat Orchestrator API (Port 5005)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ollama API    â”‚   Strands API   â”‚   SQLite DB     â”‚
â”‚  (Port 11434)   â”‚  (Port 5004)    â”‚  (Sessions)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ **Three Chat Types Supported**

### 1. **Direct LLM Chat** ğŸ¤–
- **Purpose**: Raw conversation with Ollama models
- **Intelligence**: Automatic agent routing when confidence > 70%
- **Features**:
  - Model selection (qwen2.5, llama3.2, etc.)
  - Temperature and token control
  - Custom system prompts
  - **Smart routing**: Analyzes queries and routes to appropriate agents
  - Fallback to direct LLM if routing fails

### 2. **Independent Chat Agent** ğŸ‘¤
- **Purpose**: Specialized chat agent with custom personality
- **Features**:
  - Custom role and personality
  - Capability definitions
  - Guardrails and safety controls
  - Persistent agent persona
  - Tool integration support

### 3. **Palette Agent Chat** ğŸ”—
- **Purpose**: Direct chat with existing workflow agents
- **Features**:
  - Integration with Strands agent palette
  - Context sharing with workflows
  - Tool execution through agents
  - Workflow trigger capabilities

---

## ğŸ§  **Intelligent Agent Routing**

The system includes an **AgentRouter** that analyzes user queries and determines the best agent to handle them:

```python
# Example routing decision
{
    "agent_id": "security-expert-123",
    "confidence": 0.85,
    "reasoning": "Query about network security requires cybersecurity expertise",
    "tools_needed": ["network-scanner", "vulnerability-checker"]
}
```

### Routing Logic:
1. **Query Analysis**: Uses a fast LLM (qwen2.5) to analyze user intent
2. **Agent Matching**: Compares query against available agent descriptions
3. **Confidence Scoring**: Only routes if confidence > 70%
4. **Fallback**: Falls back to direct LLM if routing fails

---

## ğŸ’¾ **Data Persistence**

### SQLite Database Schema:
```sql
-- Chat sessions
chat_sessions (id, chat_type, config, created_at, last_activity, status)

-- Chat messages  
chat_messages (id, session_id, role, content, timestamp, metadata)

-- Agent routing history
agent_routes (id, session_id, message_id, agent_id, confidence, reasoning, tools_used)
```

### Features:
- **Session Management**: Persistent chat sessions
- **Message History**: Full conversation context
- **Routing Analytics**: Track routing decisions and performance
- **Metadata Storage**: Rich message metadata (tokens, timing, tools)

---

## ğŸ”Œ **API Endpoints**

### Core Endpoints:
```bash
# Health and configuration
GET  /health                           # Service health check
GET  /api/chat/models                  # Available Ollama models
GET  /api/chat/agents                  # Available palette agents

# Session management
POST /api/chat/sessions                # Create new chat session
GET  /api/chat/sessions                # List all sessions
GET  /api/chat/sessions/{id}           # Get session info

# Chat operations
POST /api/chat/sessions/{id}/messages  # Send message
GET  /api/chat/sessions/{id}/history   # Get chat history
```

### Example Usage:
```javascript
// Create Direct LLM session
const session = await chatOrchestratorService.createDirectLLMSession({
  name: "AI Assistant",
  model: "qwen2.5:latest",
  temperature: 0.7,
  systemPrompt: "You are a helpful AI assistant with access to specialized agents."
});

// Send message
const response = await chatOrchestratorService.sendMessage(session, "How do I secure my network?");
// â†’ Automatically routes to security expert agent if available
```

---

## ğŸ¨ **Frontend Integration**

### New Components:
1. **`ChatOrchestratorService.ts`** - Frontend service layer
2. **`EnhancedFlexibleChatInterface.tsx`** - Rich chat UI with routing info
3. **Updated `ChatInterfaceNode.tsx`** - Uses new backend

### Features:
- **Real-time Status**: Connection status, processing indicators
- **Rich Metadata**: Shows routing decisions, agent info, tools used
- **Session Management**: Persistent sessions across page reloads
- **Error Handling**: Graceful fallbacks and error recovery

---

## ğŸš€ **Getting Started**

### 1. Install Dependencies:
```bash
pip3 install flask flask-cors requests
```

### 2. Start Services:
```bash
# Start Ollama (required)
ollama serve

# Start Strands API (optional, for palette agents)
cd backend && python3 strands_api.py

# Start Chat Orchestrator
./start-chat-orchestrator.sh
```

### 3. Frontend Configuration:
The frontend automatically connects through the Vite proxy:
```typescript
// vite.config.ts
'/api/chat': {
  target: 'http://localhost:5005',
  changeOrigin: true,
  secure: false,
}
```

---

## ğŸ¯ **User Experience Flow**

### Direct LLM Chat:
1. User creates "Direct LLM Chat" â†’ Selects model and settings
2. User asks: *"How do I secure my network?"*
3. **System analyzes query** â†’ High confidence for security agent
4. **Routes to Security Expert** â†’ Agent provides specialized response
5. **Fallback available** â†’ If agent fails, uses direct LLM

### Independent Agent:
1. User creates "Customer Support Bot" â†’ Defines personality and role
2. **Agent maintains persona** throughout conversation
3. **Consistent responses** based on defined capabilities
4. **Guardrails active** if enabled

### Palette Agent:
1. User selects existing agent from palette
2. **Direct integration** with Strands workflow system
3. **Tool execution** through agent's configured tools
4. **Context sharing** with workflow if enabled

---

## ğŸ”§ **Configuration Examples**

### Direct LLM Configuration:
```json
{
  "type": "direct-llm",
  "name": "AI Assistant",
  "model": "qwen2.5:latest",
  "temperature": 0.7,
  "maxTokens": 1000,
  "systemPrompt": "You are a helpful assistant with access to specialized agents."
}
```

### Independent Agent Configuration:
```json
{
  "type": "independent-agent",
  "name": "Customer Support Bot",
  "role": "Customer Service Specialist",
  "model": "llama3.2:latest",
  "personality": "Professional, empathetic, and solution-focused",
  "capabilities": ["Problem Solving", "Product Knowledge", "Escalation"],
  "guardrails": true
}
```

### Palette Agent Configuration:
```json
{
  "type": "palette-agent",
  "name": "Security Consultant",
  "agentId": "security-expert-123",
  "chatMode": "workflow-aware",
  "contextSharing": true,
  "workflowTrigger": false
}
```

---

## ğŸ“Š **Monitoring and Analytics**

### Built-in Metrics:
- **Routing Accuracy**: Track successful agent routing
- **Response Times**: Monitor generation and execution times
- **Token Usage**: Track token consumption across models
- **Agent Performance**: Success rates and error patterns
- **User Engagement**: Session duration and message counts

### Database Queries:
```sql
-- Routing success rate
SELECT 
  COUNT(*) as total_routes,
  AVG(confidence) as avg_confidence
FROM agent_routes 
WHERE created_at > datetime('now', '-1 day');

-- Most used agents
SELECT 
  agent_id, 
  COUNT(*) as usage_count
FROM agent_routes 
GROUP BY agent_id 
ORDER BY usage_count DESC;
```

---

## ğŸ”® **Future Enhancements**

### Planned Features:
1. **Streaming Responses** - Real-time message streaming
2. **Multi-Agent Conversations** - Coordinate multiple agents
3. **Context Memory** - Long-term conversation memory
4. **Custom Tools** - User-defined tool integration
5. **Voice Integration** - Speech-to-text and text-to-speech
6. **Analytics Dashboard** - Visual routing and performance metrics

### Scalability:
- **Redis Integration** - Distributed session storage
- **Load Balancing** - Multiple orchestrator instances
- **Agent Pools** - Dedicated agent worker processes
- **Caching Layer** - Response and routing caching

---

## ğŸ‰ **Benefits Over Previous System**

### Before (Proxy):
- âŒ Simple request forwarding
- âŒ No intelligence or routing
- âŒ No session management
- âŒ Limited agent integration
- âŒ No conversation context

### After (Chat Orchestrator):
- âœ… **Intelligent query routing**
- âœ… **Persistent chat sessions**
- âœ… **Rich agent integration**
- âœ… **Conversation context and memory**
- âœ… **Performance monitoring**
- âœ… **Graceful error handling**
- âœ… **Extensible architecture**

---

This new architecture transforms the chat system from a simple interface into a **sophisticated AI orchestration platform** that can intelligently route conversations, maintain context, and provide rich user experiences across all chat types! ğŸš€